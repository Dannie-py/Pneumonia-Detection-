{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "15u7HDp3cQ-Upx52jUlxjxs8pwCtoNpUX",
      "authorship_tag": "ABX9TyMwqwyLEG8Hereuit0S0XGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dannie-py/Pneumonia-Detection-/blob/main/Pneumonia_DetectionPro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount GitHub Reprository"
      ],
      "metadata": {
        "id": "Fwir6UvWi0Uh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sov7uFzUh6Ub",
        "outputId": "b6326ae2-87ce-4c6a-babf-ff4cb5db0690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'repository'...\n",
            "remote: Repository not found.\n",
            "fatal: repository 'https://github.com/Dannie-py/repository.git/' not found\n"
          ]
        }
      ],
      "source": [
        "!git clone https://Dannie-py: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@github.com/Dannie-py/repository.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Dannie-py/Pneumonia-Detection-/repository.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2OTzMd4qoG2",
        "outputId": "fa567135-4a9e-4388-ebee-e4271ecaaa1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'repository'...\n",
            "remote: Not Found\n",
            "fatal: repository 'https://github.com/Dannie-py/Pneumonia-Detection-/repository.git/' not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Dannie-Proj/Pneumonia_detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz4zDzutyQjQ",
        "outputId": "a8efc4f3-3e3e-4c72-bb1f-e0fd40dc1c59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Pneumonia_detection' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd repository\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rg7ESBHpAoz",
        "outputId": "839cdc75-6cfb-4e5a-c825-3efdcdde82fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'repository'\n",
            "/content\n",
            "drive  Pneumonia_detection  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Pneumonia-Detection-\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNKfttdVyZV2",
        "outputId": "af1ff27f-49f1-4116-d478-1268cca67fa3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Pneumonia-Detection-'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0P8nDvMyrtn",
        "outputId": "00d17d95-3c33-4154-a9da-f7fcf506a150"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  Pneumonia_detection  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Install Dependencies"
      ],
      "metadata": {
        "id": "8sMhFYhu5zpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs TensorFlow and Keras (deep learning libraries) Needed to define, train, and evaluate CNN models."
      ],
      "metadata": {
        "id": "TloKG0se5-ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow"
      ],
      "metadata": {
        "id": "DZB66eNr47EW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras"
      ],
      "metadata": {
        "id": "sZWbmvrU5Son"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-addons"
      ],
      "metadata": {
        "id": "kBCyY8OR5tZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b58ead66-75f5-405e-8a25-966b45b2e6bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries Brings in model-building tools, layers, and image utilities for preprocessing and training."
      ],
      "metadata": {
        "id": "HJpOvtR673N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf\n",
        "import numpy as np # for numerical operations\n",
        "import tensorflow as tf # tensorflow framework\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D # for building models\n",
        "from tensorflow.keras import layers, models # for building models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # for data augmentation\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121, EfficientNetB0, MobileNetV2, InceptionV3  # imports pretrained models\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import cv2 #for image processing\n",
        "import os # for path operations\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix # evaluation metrics\n",
        "from fpdf import FPDF # for creating pdf reports"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vZIGWPLrV47",
        "outputId": "f8a6f3fe-a066-4d43-fbab-116c8b05e1de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=981423246595d96463af6df305f5c39a7b7da149fe9d65aa73e73a5707e1c6a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting the dataset into Train, Val and Test data"
      ],
      "metadata": {
        "id": "o9bUC2oCDbX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules for file operations and data splitting\n",
        "import os, shutil\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "So3v2ah_EiHf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# split whole dataset into percentages of Train:70%, Val:15% and Test:15%"
      ],
      "metadata": {
        "id": "mDhKoON0FRr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original dataset folder\n",
        "original_dataset_dir = '/content/drive/MyDrive/Colab Notebooks/Data/images'\n",
        "target_base_dir = '/content/drive/MyDrive/Colab Notebooks/Data/images'\n",
        "classes = ['COVID', 'Normal', 'Viral Pneumonia']\n",
        "\n",
        "# Create target folders\n",
        "for split in ['train', 'val', 'test']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(target_base_dir, split, cls), exist_ok=True)\n",
        "\n",
        "# Split and copy images\n",
        "for cls in classes:\n",
        "    src_dir = os.path.join(original_dataset_dir, cls)\n",
        "    images = os.listdir(src_dir)\n",
        "    train, temp = train_test_split(images, test_size=0.3, random_state=42)\n",
        "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    for img in train:\n",
        "        shutil.copy(os.path.join(src_dir, img), os.path.join(target_base_dir, 'train', cls, img))\n",
        "    for img in val:\n",
        "        shutil.copy(os.path.join(src_dir, img), os.path.join(target_base_dir, 'val', cls, img))\n",
        "    for img in test:\n",
        "        shutil.copy(os.path.join(src_dir, img), os.path.join(target_base_dir, 'test', cls, img))"
      ],
      "metadata": {
        "id": "47zx9CqbFbDi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the paths for the dataset"
      ],
      "metadata": {
        "id": "-QTw5DOkgtPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting data path to train, Val and test folders\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/images\"\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "val_dir = os.path.join(data_dir, 'val')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "image_size = (224, 224) # depending of the model, image will be resized later on in the main building code\n",
        "batch_size = 16\n",
        "num_classes = 3\n",
        "class_names = ['COVID', 'Normal', 'Viral Pneumonia']\n"
      ],
      "metadata": {
        "id": "0e9nxHuDgK4C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image data generators to Rescale image pixel values, Augments images (e.g., flipping, zooming) to reduce overfitting and creates batches of images for training/validation."
      ],
      "metadata": {
        "id": "vP5nTVRXr6cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# google drive can be mounted with the follwoing codes if not using the mounting icon on the file browser on the left.\n",
        "#from google.colab import drive # to mount google drive.\n",
        "#drive.mount('/content/drive/MyDrive/Colab Notebooks/Data/images') # mount Google Drive to access the dataset\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/images\" # setting the dataset path\n",
        "\n",
        "# Image preprocessing\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'train'),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'val'),\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'test'),\n",
        "    target_size=img_size,\n",
        "    batch_size=1,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gozDeKQRr_OM",
        "outputId": "cf8a0a38-9217-4611-b183-a15800123e0d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5XgEkQnywoLR"
      }
    }
  ]
}